{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import cfg as c\n",
    "import custom_functions as cfoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484f81e2ef4489f84d3a16a79f83a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model & tokenizer (meta-llama/Llama-3.1-8B-Instruct saved to: /home/adi/projects/projectx/dev_models)\n"
     ]
    }
   ],
   "source": [
    "# set-up GPU quantization \n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# get model and tokenizer (will download from hf_hub if not existing)\n",
    "tokenizer = AutoTokenizer.from_pretrained(c.MODEL, \n",
    "                                          trust_remote_code=True, \n",
    "                                          cache_dir=cfoos.check_create_dir(c.MODELS_DEV_PATH))\n",
    "model = AutoModelForCausalLM.from_pretrained(c.MODEL, \n",
    "                                             trust_remote_code=True, \n",
    "                                             cache_dir=cfoos.check_create_dir(c.MODELS_DEV_PATH),\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=quant_config)\n",
    "\n",
    "print(f\"---Model & tokenizer ({c.MODEL} saved to: {c.MODELS_DEV_PATH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory (GB): 29.92\n"
     ]
    }
   ],
   "source": [
    "# check size of quantized model (bytes > GB)\n",
    "cfoos.check_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Saving quantized model...\n",
      "---Saved model to: /home/adi/projects/projectx/quant_models/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# save GPU quantized model\n",
    "cfoos.save_quant_model(model=model,\n",
    "                       tokenizer=tokenizer,\n",
    "                       # create quant model directory if it doesn't exist\n",
    "                       model_path=cfoos.check_create_dir(c.MODELS_QUANT_PATH / c.MODEL_NAME),\n",
    "                       # # uncomment to empyt dev_models\n",
    "                       # clear_dev_models_path=c.MODELS_DEV_PATH\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4c2f0672444efb820b2d3a948202c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf:   0%|          | 0.00/4.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/adi/projects/projectx/quant_models/Meta-Llama-3.1-8B-Instruct-GGUF/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a pre-existing CPU quantized model\n",
    "hf_hub_download(\n",
    "    repo_id=c.MODEL_CPU, \n",
    "    filename=\"Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf\",\n",
    "    cache_dir=cfoos.check_create_dir(c.MODELS_DEV_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Saving cpu-quantized model...\n",
      "---Saved model to: /home/adi/projects/projectx/quant_models/Meta-Llama-3.1-8B-Instruct-GGUF\n"
     ]
    }
   ],
   "source": [
    "# save the cpu model in production folder\n",
    "cfoos.save_quant_cpu_model(\n",
    "    model_name=c.MODEL_NAME_CPU,\n",
    "    dev_models_dir=c.MODELS_DEV_PATH,\n",
    "    # create quant model directory if it doesn't exist\n",
    "    model_path=cfoos.check_create_dir(c.MODELS_QUANT_PATH / c.MODEL_NAME_CPU),\n",
    "    # # uncomment to empyt dev_models\n",
    "    # clear_dev_models_dir: c.MODELS_DEV_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding model \n",
    "emb_model = SentenceTransformer(c.MODEL_EMB, trust_remote_code=True, cache_folder=c.MODELS_DEV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Saved embedding model to: /home/adi/projects/projectx/quant_models/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# save embedding model\n",
    "cfoos.save_emb_model(\n",
    "    c.MODEL_NAME_EMB,\n",
    "    c.MODELS_DEV_PATH,\n",
    "    cfoos.check_create_dir(c.MODELS_QUANT_PATH / c.MODEL_NAME_EMB)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

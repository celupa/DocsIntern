{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import io\n",
    "import logging\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gradio as gr\n",
    "from helpers import cfg as c\n",
    "from helpers.model_selector import ModelSelector\n",
    "from helpers.rag_handler import RAGHandler\n",
    "from helpers.evaluator import Evaluator\n",
    "from helpers.tuner import Tuner\n",
    "from helpers.flow_handler import FlowHandler\n",
    "from helpers import custom_functions as cfoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: someone should put me down because I'm committing war crimes again (re-work the module, you bloody benchod!!)\n",
    "# redirect logging to log buffer\n",
    "LOG_BUFFER = io.StringIO()\n",
    "stream_handler = logging.StreamHandler(LOG_BUFFER)\n",
    "cfoos._set_log_stream(cfoos.LOG, stream_handler)\n",
    "# prepare a separate thread to timeout error coming from gradio \n",
    "EXECUTOR = ThreadPoolExecutor()\n",
    "PARSING_STATUS = \"done\"\n",
    "# preconfigure app\n",
    "MODEL_SELECTOR = ModelSelector(c.MODEL_CONFIG)\n",
    "RAG_HANDLER = None\n",
    "FLOW_HANDLER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: troubleshooting\n",
    "# INIT_RAG = True\n",
    "# PARSE = True\n",
    "# EVALUATE = True\n",
    "# TUNING = True\n",
    "# if INIT_RAG:\n",
    "#     from pathlib import Path\n",
    "#     data_path = Path(\"/home/adi/projects/DocsIntern/doc_folder\")\n",
    "#     sub_dir = Path(\"/home/adi/projects/DocsIntern/doc_folder/écriture/poèmes\")\n",
    "#     uniform_data = Path(\"/home/adi/projects/DocsIntern/knowledge-base\")\n",
    "#     nsource = None\n",
    "#     rag_handler = RAGHandler(\n",
    "#         db_path=c.DB_PATH,\n",
    "#         data_path=sub_dir,\n",
    "#         supported_extensions=c.SUPPORTED_EXTENSIONS,\n",
    "#         collection_config=c.COLLECTION_CONFIG,\n",
    "#         collection_name=c.DB_COLLECTION,\n",
    "#         model=MODEL_SELECTOR\n",
    "#     )\n",
    "\n",
    "# if PARSE:\n",
    "#     data = rag_handler.extract_data()\n",
    "#     chunks = rag_handler.get_chunks(\n",
    "#         extracted_data=data, \n",
    "#         chunk_size_factor=1.0,\n",
    "#         chunk_overlap_factor=0.2)\n",
    "#     # rag_handler.populate_vectdb(chunks, collection_config)\n",
    "#     rag_handler.populate_vectdb(chunks)\n",
    "\n",
    "# if EVALUATE:\n",
    "#     import pandas as pd \n",
    "#     evaluator = Evaluator(MODEL_SELECTOR, rag_handler)\n",
    "#     df = evaluator.get_eval_df()\n",
    "#     # # TODO: adjust args for eval_df\n",
    "#     eval_df = evaluator.generate_eval_prompts(\n",
    "#         df=df,\n",
    "#         read_local_df=True,\n",
    "#         save_df=False\n",
    "#         )\n",
    "#     anal_df = evaluator.prompt_db(eval_df)\n",
    "#     evaluator.analyze_retrievals(anal_df)\n",
    "\n",
    "# if TUNING:\n",
    "#     tuner = Tuner(\n",
    "#         model=MODEL_SELECTOR,\n",
    "#         rag_handler=rag_handler,\n",
    "#         evaluator=evaluator,\n",
    "#         tuning_config=c.TUNING_MODIFIERS,\n",
    "#     )\n",
    "#     # tuning_df = tuner.tune_retrieval(save_df=True)\n",
    "#     tuning_df = tuner.tune_retrieval(save_df=False, short=True)\n",
    "#     # tuner.select_user_config(tuning_df)\n",
    "#     tuner.select_user_config(tuning_df, manual_opt=\"speed\")\n",
    "\n",
    "# flow_handler = FlowHandler(\n",
    "#     model=MODEL_SELECTOR,\n",
    "#     rag_handler=rag_handler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_console() -> str:\n",
    "    \"\"\"Output \"logs\" to console, but truncate if too many lines.\"\"\"\n",
    "    lines = LOG_BUFFER.getvalue().split(\"\\n\")\n",
    "    nlines = len(LOG_BUFFER.getvalue())\n",
    "    # limit output lines\n",
    "    if nlines >= 46:\n",
    "        console_lines = lines[-46:]\n",
    "        console = \"\\n\".join(console_lines) \n",
    "        LOG_BUFFER.truncate(0)\n",
    "        LOG_BUFFER.seek(0)\n",
    "        LOG_BUFFER.write(console)\n",
    "        return LOG_BUFFER.getvalue()\n",
    "    return LOG_BUFFER.getvalue()\n",
    "\n",
    "def prime_flow(data_path: str=None, tune: str=None) -> None:\n",
    "    \"\"\"\n",
    "    Initialize application depending on state.\n",
    "    params:\n",
    "        data_path: path to data\n",
    "        tune: if true, app will find most optimal params (rag retrieval, chunks...)\n",
    "        \"\"\"\n",
    "    global RAG_HANDLER\n",
    "    global FLOW_HANDLER\n",
    "    # if data has been parsed before, init_config.txt will contain previous path\n",
    "    # this is a precursor to future updates\n",
    "    init_config_path = Path(c.APP_DIR / \"init_config.txt\")\n",
    "    # if data_path and tuning has been provided by the user, then begin parsing flow\n",
    "    if data_path:\n",
    "        RAG_HANDLER = RAGHandler(\n",
    "            db_path=c.DB_PATH,\n",
    "            data_path=data_path,\n",
    "            supported_extensions=c.SUPPORTED_EXTENSIONS,\n",
    "            collection_config=c.COLLECTION_CONFIG,\n",
    "            collection_name=c.DB_COLLECTION,\n",
    "            model=MODEL_SELECTOR\n",
    "            )\n",
    "        data = RAG_HANDLER.extract_data()\n",
    "        chunks = RAG_HANDLER.get_chunks(\n",
    "            extracted_data=data, \n",
    "            chunk_size_factor=1.0,\n",
    "            chunk_overlap_factor=0.2)\n",
    "        RAG_HANDLER.populate_vectdb(chunks)\n",
    "        if tune == \"Yes\":\n",
    "            # get a baseline evaluation\n",
    "            evaluator = Evaluator(MODEL_SELECTOR, RAG_HANDLER)\n",
    "            df = evaluator.get_eval_df()\n",
    "            eval_df = evaluator.generate_eval_prompts(\n",
    "                df=df,\n",
    "                read_local_df=False,\n",
    "                save_df=False\n",
    "                )\n",
    "            anal_df = evaluator.prompt_db(eval_df)\n",
    "            evaluator.analyze_retrievals(anal_df)\n",
    "            tuner = Tuner(\n",
    "                model=MODEL_SELECTOR,\n",
    "                rag_handler=RAG_HANDLER,\n",
    "                evaluator=evaluator,\n",
    "                tuning_config=c.TUNING_MODIFIERS,\n",
    "                )\n",
    "            # tune baseline evaluation\n",
    "            tuning_df = tuner.tune_retrieval(save_df=False)\n",
    "            # ftm, we default to speed \n",
    "            tuner.select_user_config(tuning_df, manual_opt=\"speed\")\n",
    "    # if user didn't opt for parsing, fetch existing config file and get RAGHandler\n",
    "    if init_config_path.exists():\n",
    "        with open(init_config_path, \"r\") as f:\n",
    "            stored_data_path = f.read()\n",
    "        RAG_HANDLER = RAGHandler(\n",
    "            db_path=c.DB_PATH,\n",
    "            data_path=stored_data_path,\n",
    "            supported_extensions=c.SUPPORTED_EXTENSIONS,\n",
    "            collection_config=c.COLLECTION_CONFIG,\n",
    "            collection_name=c.DB_COLLECTION,\n",
    "            model=MODEL_SELECTOR\n",
    "            )\n",
    "    else:\n",
    "        RAG_HANDLER = RAGHandler(\n",
    "            db_path=c.DB_PATH,\n",
    "            data_path=\"None\",\n",
    "            supported_extensions=c.SUPPORTED_EXTENSIONS,\n",
    "            collection_config=c.COLLECTION_CONFIG,\n",
    "            collection_name=c.DB_COLLECTION,\n",
    "            model=MODEL_SELECTOR\n",
    "            )\n",
    "    FLOW_HANDLER = FlowHandler(\n",
    "        model=MODEL_SELECTOR,\n",
    "        rag_handler=RAG_HANDLER\n",
    "        )\n",
    "        \n",
    "def interaction_on(*args: str) -> Tuple[gr.update]:\n",
    "    \"\"\"Turn widget interaction on.\"\"\"\n",
    "    return [gr.update(interactive=True) for _ in range(len(args))]\n",
    "\n",
    "def interaction_off(*args: str) -> Tuple[gr.update]:\n",
    "    \"\"\"Turn widget interaction off.\"\"\"\n",
    "    return [gr.update(interactive=False) for _ in range(len(args))]\n",
    "\n",
    "def format_path(text):\n",
    "    # format linux/mac path\n",
    "    linux_mac_regex = r'/[^\\s:*?\"<>|]+'  \n",
    "    # format windows paths\n",
    "    windows_regex = r'[a-zA-Z]:\\\\[^\\s:*?\"<>|]+' \n",
    "    # format unc network path\n",
    "    unc_regex = r'\\\\\\\\[^\\s:*?\"<>|]+'  \n",
    "\n",
    "    pattern = f\"({linux_mac_regex})|({windows_regex})|({unc_regex})\"\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        return next(filter(None, match.groups())) \n",
    "    return None\n",
    "\n",
    "def validate_path(data_path=None, tune: str=None) -> Tuple[dict, dict]:\n",
    "    \"\"\"Validate data path and control app flow.\"\"\"\n",
    "    global PARSING_STATUS\n",
    "    path_warning = \"Hey, you! Please check the provided path exists, contains valid files and is formatted correctly...\"\n",
    "    try:\n",
    "        data_path = format_path(data_path)\n",
    "        if not Path(data_path).exists():\n",
    "            data_path = gr.update(label=path_warning, value=\"\")\n",
    "            return data_path\n",
    "        PARSING_STATUS = \"working\"\n",
    "        future = EXECUTOR.submit(prime_flow, data_path, tune)\n",
    "        future.result()\n",
    "        data_path = gr.update(label=\"Data Path\", value=\"\")\n",
    "        PARSING_STATUS = \"done\"\n",
    "        return data_path\n",
    "    except ValueError:\n",
    "        PARSING_STATUS = \"done\"\n",
    "        data_path = gr.update(label=path_warning, value=\"\")\n",
    "        return data_path\n",
    "\n",
    "def chat(prompt, history):\n",
    "    \"\"\"Support gradio with history building.\"\"\"\n",
    "    response, data_info = FLOW_HANDLER.prompt_llm(prompt)\n",
    "    if history is None:\n",
    "        history = []\n",
    "    if data_info:\n",
    "        response = f\"{response}\\n{data_info}\"\n",
    "    history.append(({\"role\": \"user\", \"content\": prompt}))\n",
    "    history.append(({\"role\": \"assistant\", \"content\": response}))\n",
    "    # empty chat input text\n",
    "    chat_input = gr.Textbox(value=\"\")\n",
    "    # one history for the chatbot and one for the state\n",
    "    return history, history, chat_input\n",
    "\n",
    "prime_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable tuning for CPUs\n",
    "tune_state = False if MODEL_SELECTOR.device == \"cpu\" else True\n",
    "tune_text = \"Tune (CUDA required)\" if MODEL_SELECTOR.device == \"cpu\" else \"Tune\"\n",
    "# setup gradio\n",
    "with gr.Blocks() as app:\n",
    "    # data parse logic and chatbot\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            data_path = gr.Textbox(label=\"Data Path\")\n",
    "            tune = gr.Dropdown(\n",
    "                [\"No\", \"Yes\"], \n",
    "                label=tune_text, \n",
    "                value=\"No\",\n",
    "                interactive=tune_state\n",
    "                )\n",
    "            parse_button = gr.Button(\"Parse\")\n",
    "            chatbot = gr.Chatbot(label=\"Chatbot\", type=\"messages\")\n",
    "            chat_input = gr.Textbox(show_label=False, placeholder=\"Ask me something\")\n",
    "            chat_button = gr.Button(\"Chat\")\n",
    "            parse_button.click(interaction_off, \n",
    "                               inputs=[parse_button, chat_input, chat_button], \n",
    "                               outputs=[parse_button, chat_input, chat_button]\n",
    "                               ).then(validate_path, \n",
    "                                      inputs=[data_path, tune], \n",
    "                                      outputs=[data_path]\n",
    "                                      ).then(interaction_on, \n",
    "                                             inputs=[parse_button, chat_input, chat_button], \n",
    "                                             outputs=[parse_button, chat_input, chat_button]\n",
    "                                             )\n",
    "            state = gr.State([])\n",
    "            chat_button.click(chat, inputs=[chat_input, state], outputs=[chatbot, state, chat_input])\n",
    "        with gr.Column(scale=1):\n",
    "            console = gr.Code(\n",
    "            interactive=False,\n",
    "            lines=46,\n",
    "            max_lines=46,\n",
    "            label=\"Console\",\n",
    "            value=\"\"\n",
    "        )\n",
    "    timer = gr.Timer(0.1, active=True)\n",
    "    timer.tick(update_console, outputs=console)\n",
    "\n",
    "# app.queue().launch(inbrowser=True)\n",
    "app.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
